{
  # Spark relation config
  spark: {
    app: {
      name: Spark Writer
    }

    driver: {
      cores: 1
      maxResultSize: 1G
    }

    cores {
      max: 16
    }
  }

  # Nebula Graph relation config
  nebula: {
    addresses: ["127.0.0.1:3699"]
    user: user
    pswd: password
    space: test

    connection {
      timeout: 3000
      retry: 3
    }

    execution {
      retry: 3
    }

    error: {
      max: 32
      output: /tmp/errors
    }

    rate: {
      limit: 64M
      timeout: 1000
    }
  }

  # Processing tags
  tags: [

    # Loading tag from HDFS and data type is parquet
    {
      name: tag-name-0
      type: parquet
      path: hdfs tag path 0
      fields: {
        hive-field-0: nebula-field-0,
        hive-field-1: nebula-field-1,
        hive-field-2: nebula-field-2
      }
      vertex: hive-field-0
      batch: 2
      separator: ","
      header: true
    }

    # Loading from Hive
    {
      name: tag-name-1
      type: hive
      exec: "select hive-field0, hive-field1, hive-field2 from database.table"
      fields: {
        hive-field-0: nebula-field-0,
        hive-field-1: nebula-field-1,
        hive-field-2: nebula-field-2
      }
      vertex: {
        field: hive-field-0
        policy: "hash"
      }
      vertex: hive-field-0
      partition: 32
    }

    # Loading from Neo4J
    {
      name:        tag-name-2
      server:      "bolt://127.0.0.1:7687"
      user:        neo4j
      password:    neo4j
      source.type: neo4j
      sink.type:   client
      fields: {
        neo4j-field-0: nebula-field-0,
        neo4j-field-1: nebula-field-1,
        neo4j-field-2: nebula-field-2
      }
      vertex:      neo4j-field-0
      partition:   5
      batch:       2000
      exec:        "match (n:tag-name-2) return n.f0 as neo4j-field-0, n.f1 as neo4j-field-0, n.f2 as neo4j-field-2"
      check_point: "/tmp"
    }
  ]

  # Processing edges
  edges: [
    # Loading tag from HDFS and data type is parquet
    {
      name: edge-name-0
      type: json
      path: hdfs edge path 0
      fields: {
        hive-field-0: nebula-field-0,
        hive-field-1: nebula-field-1,
        hive-field-2: nebula-field-2
      }
      source: {
        field: hive-field-0
        policy: "hash"
      }
      target: {
        field: hive-field-1
        policy: "uuid"
      }
      ranking: hive-field-2
      partition: 32
    }

    # Loading from Hive
    {
      name: edge-name-1
      type: hive
      exec: "select hive-field0, hive-field1, hive-field2 from database.table"
      fields: {
        hive-field-0: nebula-field-0,
        hive-field-1: nebula-field-1,
        hive-field-2: nebula-field-2
      }
      source: hive-field-0
      target: hive-field-1
      partition: 32
    }

    # Loading from Neo4J
    {
      name:        edge-name-2
      source.type: neo4j
      sink.type:   client
      server:      "bolt://127.0.0.1:7687"
      user:        neo4j
      password:    neo4j
      fields: {
        neo4j-field-0: nebula-field-0,
        neo4j-field-1: nebula-field-1,
        neo4j-field-2: nebula-field-2
      }
      source: {
        field: neo4j-field-0
      }
      target: {
        field: neo4j-field-1
      }

      pre-partition: 5
      batch:         2000
      exec:          "match (start:tag_start)-[edge:edge-name-2]->(end:tag_end) return edge.f0 as neo4j-field-0, edge.f1 as neo4j-field-1, edge.f2 as neo4j-field-2"
      check_point:   "/tmp"
    }
  ]
}
