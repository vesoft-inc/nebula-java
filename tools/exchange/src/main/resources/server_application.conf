{
  # Spark relation config
  spark: {
    app: {
      name: Spark Writer
    }

    driver: {
      cores: 1
      maxResultSize: 1G
    }

    cores {
      max: 16
    }
  }

  # Nebula Graph relation config
  nebula: {
    address:{
      graph:["127.0.0.1:3699"]
      meta:["127.0.0.1:45500"]
    }
    user: user
    pswd: password
    space: test

    connection {
      timeout: 3000
      retry: 3
    }

    execution {
      retry: 3
    }

    error: {
      max: 32
      output: /tmp/errors
    }

    rate: {
      limit: 1024
      timeout: 1000
    }
  }

  # Processing tags
  tags: [

    # Loading tag from neo4j
    {
      name: tag-name-0
      type: {
        source: neo4j
        sink: client
      }
      server: "bolt://127.0.0.1:7687"
      user: neo4j
      password: neo4j
      exec: "match (n:label) return n.neo4j-field-0 as neo4j-field-0, n.neo4j-field-1 as neo4j-field-1 order by (n.neo4j-field-0)"
      fields: [neo4j-field-0, neo4j-field-1]
      nebula.fields: [nebula-field-0, nebula-field-1]
      vertex: {
        field:neo4j-field-0
        policy:uuid
      }
      partition: 10
      batch: 1000
      check_point_path: /tmp/test
    }

    # Loading from janus graph
    {
      name: tag-name-1
      type: {
        source: janus graph
        sink: client
      }
      host:127.0.0.1
      port:8182
      label: janus-vertex-label
      fields: [janus-field-0, janus-field-1, janus-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: janus-field-0
      partition: 10
      batch: 1000
      check_point_path: /tmp/test
    }

    # Loading from HBase, if fields or vertex contains rowkey, please configure it as rowkey.
    {
      name: hbase-table-name
      type: {
        source: hbase
        sink: client
      }
      host:127.0.0.1
      port:2181
      table:hbase-table
      columnFamily:hbase-table-cloumnfamily
      fields: [hbase-column-0, hbase-column-1]
      nebula.fields: [nebula-field-0, nebula-field-1]
      vertex: rowkey
      partition: 10
      batch: 1000
    }
  ]

  # Processing edges
  edges: [

    # Loading from neo4j
    {
      name: edge-name-0
      type: {
        source: neo4j
        sink: client
      }
      server: "bolt://127.0.0.1:7687"
      user: neo4j
      password: neo4j
      exec: "match (a:vertex_label)-[r:edge_label]->(b:vertex_label) return a.neo4j-source-field, b.neo4j-target-field, r.neo4j-field-0 as neo4j-field-0, r.neo4j-field-1 as neo4j-field-1 order by id(r)"
      fields: [neo4j-field-0, neo4j-field-1]
      nebula.fields: [nebula-field-0, nebula-field-1]
      source: {
        field: a.neo4j-source-field
        policy: "hash"
      }
      target: {
        field: b.neo4j-target-field
        policy: "uuid"
      }
      partition: 10
      batch: 1000
      check_point_path: /tmp/test
    }

    # Loading from janus graph
    {
      name: edge-name-1
      type: {
        source: janus graph
        sink: client
      }
      partition: 10
      batch: 10
      host:127.0.0.1
      port:8182
      label: janus-edge-label
      fields: [janus-field-0, janus-field-1, janus-field-2]
      nebula.fields:[nebula-field-0, nebula-field-1, nebula-field-2]
      source: {
        field: IN.id
      }
      target: {
        field: OUT.id
      }
      partition: 10
      batch: 1000
      check_point_path: /tmp/test
    }
    # Loading from hbase
    {
      name: hbase-table-name
      type: {
        source: hbase
        sink: client
      }
      host:127.0.0.1
      port:2181
      table:hbase-table
      columnFamily:hbase-table-cloumnfamily
      fields: [hbase-column-0, hbase-column-1]
      nebula.fields:[nebula-field-0, nebula-field-1]
      source: {
        field: hbase-column-k
        policy:hash
      }
      target: {
        field: hbase-column-h
        policy:hash
      }
      partition: 10
      batch: 1000
    }
  ]
}
