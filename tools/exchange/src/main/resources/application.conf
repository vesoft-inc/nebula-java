{
  # Spark relation config
  spark: {
    app: {
      name: Spark Writer
    }

    driver: {
      cores: 1
      maxResultSize: 1G
    }

    cores {
      max: 16
    }
  }

  # Nebula Graph relation config
  nebula: {
    address:{
      graph:["127.0.0.1:3699"]
      meta:["127.0.0.1:45500"]
    }
    user: user
    pswd: password
    space: test

    connection {
      timeout: 3000
      retry: 3
    }

    execution {
      retry: 3
    }

    error: {
      max: 32
      output: /tmp/errors
    }

    rate: {
      limit: 1024
      timeout: 1000
    }
  }

  # Processing tags
  tags: [

    # Loading tag from HDFS and data type is parquet
    {
      name: tag-name-0
      type: {
        source: parquet
        sink: client
      }
      path: hdfs tag path 0
      fields: [parquet-field-0, parquet-field-1, parquet-field-2]
      nebula.fields: [nebula-field-0 nebula-field-1 nebula-field-2]
      vertex: hive-field-0
      batch: 256
      partition: 32
      # isImplicit: As a switch to allow implicit conversion of data, default true
      # if isImplicit is true, the source field's datatype will be convert according to nebula data type.
      #
      # for Nebula String type
      #     1. empty String will be convert to ""
      #     2. Short, Int, Long, Decimal, Float, Double will be convert to String
      #     3. Boolean true will be convert to String as format "true"
      #     4. List/Array will be convert to String as format: [value1,value2]
      #     5. Map will be convert to String as format: {key1:value1,key2:value2}
      # for Nebula Long or Double Type
      #     1. numberic String will be convert to Long or Double
      #     2. Short, Int, Long, Decimal, Float, Double will be convert to Long or Double
      #     3. List/Array/Map will throw exception
      # for Nebula Boolean Type
      #     1. only String True/true/Flase/false can be convert to Boolean
      isImplicit: true
    }

    # Loading from Hive
    {
      name: tag-name-1
      type: {
        source: hive
        sink: client
      }
      exec: "select hive-field0, hive-field1, hive-field2 from database.table"
      fields: [hive-field-0, hive-field-1, hive-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field: hive-field-0
        policy: "hash"
      }
      vertex: hive-field-0
      batch: 256
      partition: 32
      isImplicit: true
    }

    # Loading tag from HDFS and data type is csv
    {
      name: tag-name-2
      type: {
        source: csv
        sink: client
      }
      path: hdfs tag path 2
      fields: [csv-field-0, csv-field-1, csv-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: csv-field-0
      separator: ","
      header: false
      csv.fields: [csv-field-0,csv-field-1,csv-field-2]
      batch: 256
      partition: 32
      isImplicit: true
    }
  ]

  # Processing edges
  edges: [
    # Loading tag from HDFS and data type is json
    {
      name: edge-name-0
      type: {
        source: json
        sink: client
      }
      path: hdfs edge path 0
      fields: [json-field-0, json-field-1, json-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: {
        field: hive-field-0
        policy: "hash"
      }
      target: {
        field: hive-field-1
        policy: "uuid"
      }
      ranking: hive-field-2
      batch: 256
      partition: 32
      isImplicit: true
    }

    # Loading from Hive
    {
      name: edge-name-1
      type: {
        source: hive
        sink: client
      }
      exec: "select hive-field0, hive-field1, hive-field2 from database.table"
      fields: [ hive-field-0, hive-field-1, hive-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: hive-field-0
      target: hive-field-1
      batch: 256
      partition: 32
    }
  ]
}
